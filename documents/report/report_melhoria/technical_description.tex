\chapter{Technical Description} \label{ch:technical}

The P8-STATS package, the integral part of the statistical core developed by the
Portuguese team consists in developing an Executive Infortmation System,
integrated in the ErasmusLine ecosystem.

Executive Information Systems (EIS) are Information Systems that access large
amounts of business related information from various sources - internal and
external to the organization - and provides decision support tools to the
organization's senior executives.

These tools allow the executive managers to highlight patterns in the
business process by analyzing, comparing and determining trends in the provided
mediums - spreadsheets, graphic charts, pivot tables, reports, etc.

As an example of EIS use, consider the following scenario:

\begin{itemize}
\item[] Erasmus executive staff and coordinators can do a periodical check on
student enrollment information, i.e. what countries do they come from and what courses
are their main choices. Using a web browser, executive memebers can access that
information in a personalized fashion by presenting subsets of the information -
views. These views can be "drilled-down" to minute levels of information or
"rolled-up" to display a broader view.
\end{itemize}

Since executive users are not necessarily data analysts, the EIS user interface
must be as simple as possible, but without sacrificing presentation dynamics.

With these criteria in mind, the following key requirements for an EIS where
identified\cite{eis:psu}:

\begin{itemize}
  \item Cross Platform
  \item Ease of Use
  \item Limited Training
  \item Quick Response
  \item Process Large Volumes of Data
  \item Deployment Through the Web
  \item Easy Graphical Presentation Options
  \item Ability to Access Subsets of Data (Drill Down)
\end{itemize}

\section{Architecture}
An EIS can be divided into three main levels of architecture\cite{eis:dlbi}, that
covers the technology used to extract, analyze and visualize information from other data
sources with a friendly and flexible user interface. The levels discussed in the
next sections are the following:

\begin{description}
  \item[Data Management] Data gathering and transformation, it's lifecycle and
  target storage.
  \item[Model Management] How the data is transformed and retrieved for
  analytical purposes.
  \item[Data Visualization] Which visual tools are used to represent the
  underlying system architecture.
\end{description}


\section{Data Management}

The Data Management Level deals with the extraction, analysis and processing of
the internal and external data sources and passes that data through an ETL
process - Extract, Transform and Load - that organizes and aggregates the data
into the Data Warehouse. The use of ETL and Data Warehouses can be more
efficient to the EIS due to it's data treatment and Warehouse database model
schemata.

In this project's context, the sole Data Source for the Data Management level is
the main Erasmusline database that provides information for 
\begin{itemize}
	\item The Students
	\item The Higher Education Institutions
	\item The lectured Courses
	\item Student Forms
	\item Exams
\end{itemize}

It is this data that will pass through the ETL stage.

\subsection{ETL process}

The ETL process parses the data and performs these possible
operations\cite{wiki:etl}:
\begin{itemize}
  \item Translation into database compatible rows / columns
  \item Translate code values
  \item Remove / Add / Translate code values
  \item Change data encoding
  \item Unify string value variations - i.e. 'Mr','Mister'; '1','One'; etc.
  \item Sorting values
  \item Data validation
  \item Summarize data
  \item Join data from various sources
  \item Generating surrogate keys
\end{itemize}

Also, ETL processes can make use of temporary database tables for intermediate
processing, called Operational Data Stores (ODS). These tables hold historical
data that can me mined for statistical information to be included in the Data
Warehouse.

\fig{img/etl_workflow.png}{ETL Process Workflow}{img:etl_workflow}{0.8}

For the P8-STATS package, the ETL process - Figure \ref{img:etl_workflow} - will
be executed either by an event trigger - saving data in the ODS’s when a certain
workflow phase is reached - and periodically - part of the data refreshment
process to aggregate new information to the Data Warehouse.

\subsubsection{Data Refreshment}

This phase of the ETL process is comprised of design choices, like data
structures, techniques to update and optimize the flow of information from the
data sources to the Data Warehouse and update cycle.

Based on the statistical overview document of the Erasmus Program of 2010 – in
the Annexes – it was verified that with the large volume of students mobility,
having a completely centralized database wouldn't be viable in terms of database
synchronization and efficiency - 198523 students got mobilized by 2747
Institutions in the academic year 2008/2009.

The process begins with the loading of information from the Operational Data
Stores.

\paragraph{Loading}

The information sent to the ETL component will be fetched from the application
data sources while other subsets of information are already stored in the ODS’s
or Metadata tables - these hold information previously gathered by the business
requisites - which is going to be processed by the ETL component and integrated
into the Data Warehouse.

To aggregate and integrate data to be viewed in the EIS, an update cycle for the
Data Warehouse must be determined. The information models that need to be
gathered over time are noted in the following table:  

\tab{img/refresh_cycle.png}{EIS update cycle for the Data Refreshment
phase}{tab:refresh_cycle}{0.7}

After the Process Efficiency data is integrated into the Institution’s local
Data Mart - a local database, part of the Data Warehouse - the Efficacy process
gathers the data from the various Institution’s Data Marts and additional data
to aggregate and integrate new data into the central Data Mart.

More on this discussion in the Deployment section, page
\pageref{sec:deployment} of this report.

\paragraph{Aggregation}

The aggregation process of Student Application data fetches the information from
the ODS database - Figure \ref{img:ods_tables} - and integrates those
values in the Data Warehouse database.

Some technical aspects have to be considered in the loading and aggregation of
data sources:

\begin{itemize}
  \item Keep an historical record of information already loaded, so as not to
  fetch the same data twice. This is a achieved with a caching mechanism, to
  prevent repetitive DB queries for example.
  \item The Data Warehouse Database Schema must be optimized for query
  performance, that includes:
  \begin{itemize}
    \item Which indices are needed
    \item The Index type vs the Data type
    \item Which Database/Table engine
   \end{itemize}
\end{itemize}

The technical considerations and decision are explained in detail on page
\pageref{ssec:mysql_merge} - \nameref{ssec:mysql_merge}.

\fig{img/ods_tables.png}{ODS and Metadata DB Schema}{img:ods_tables}{0.7}

\newpage
\subsection{Data Warehouse}

A Data Warehouse holds the processed information into a database designed
properly for the retrieval of large amounts of data. The database structure is
composed of fact tables and dimension tables, connected by their keys in a
\textbf{Star} or \textbf{Snowflake} schema\cite{dwtk}.

A Fact table holds facts - an aggregation of contextualized fields and numerical
values for measurement\cite{dwtk}.

A Dimension table holds the various combinations of a given dimension, often
identified by a part of the composite key in the fact table and serves as a
means to restrict and summarize the information contained in the fact table
("roll-up" and "drill-down" operations)\cite{dwtk}.

Figure \ref{img:dw_schema} shows the final Data Warehouse database schema used
in the ErasmusLine P8-STATS package.

This schema follows the Star schema concept, not as normalized as the Snowflake
schema but with better performance\cite{dwtk} - Snowflake schemas are in third
normal form - since it doesn't need the overhead of looking up additinal tables
in the OLAP module.

The following analysis breakdown in the next section describes which data can be
mined given the project Specifications and the ErasmusLine business information,
and what Key Performance Indicators can be obtained, measured and stored in the
Data Warehouse.

\newpage
\fig{img/dw_schema.png}{Data Warehouse Database Schema}{img:dw_schema}{0.7}
\newpage

\subsubsection{Data Warehouse Statistical Specifications}

Given the initial instructions provided by the MUTW 2011 Student's Project
Specification document, the main analytical objectives for the P8-STATS package
are the following:

\begin{itemize}
  \item Efficiency
  \item Efficacy
\end{itemize}

Given the state and final content of the main \emph{ErasmusLine} database, the
following Key Performance Indicators can be established:

\paragraph{Efficiency}
\begin{itemize}
  \item Student participation
  \item Response time between Process phases
  \item Lodging availability
\end{itemize}

\paragraph{Efficacy}
\begin{itemize}
  \item Student Applications
  \item Student credits
  \item Student grades
\end{itemize}

Taking these KPI's into account, Table \ref{tab:measures} details the
Measures that could be determined.

\tab{img/tab_measures.png}{Defined Measures for the Data
Warehouse}{tab:measures}{0.6}

A cross analysis between Measures and KPI's to determine which measures would
be used more than once, shown in Table \ref{tab:kpi_measures}.

\tab{img/tab_kpi_measures.png}{Cross referencing KPI's with
Measures}{tab:kpi_measures}{0.7}

In Table \ref{tab:dimensions} are the Dimensions gathered from the
specifications document that cross reference with the final ErasmusLine
application database.

\tab{img/tab_dimensions.png}{The Dimension Classification, their levels of
detail and examples}{tab:dimensions}{0.7}

\newpage
To match which Dimension would be present on each KPI, Table \ref{tab:kpi_dim}
shows their relations.

\tab{img/tab_kpi_dim.png}{Cross reference between KPI's and
Dimensions}{tab:kpi_dim}{0.7}

With these tables into account it was possible to determine the Data Warehouse
Database schema illustrated in Figure \ref{img:dw_schema}.

Additional analysis was made to make the database schema scalable and perform
well under high server loads. The following section explains the choices of
the underlying database architecture given the system's constraints and
possibilities.

\subsubsection{MySQL and Merge Tables}\label{ssec:mysql_merge}

MySQL is a RDBMS server that provides storage of information with relational
traits, that supports multiple databases and database engines, multiple users
and threaded access. It is an open source solution and currently the most
popular relational database for small and medium sized projects. For the
P8-STATS package, the stable release MySQL 5.1 was used because it is a
generally available release and can be installed and found in any mainstream
Operating System today.

The P8-STATS package requires fast access and processing times with low
overhead, in order to serve the client as fast as possible given that the amount
of data in the Data Warehouse will grow exponentially every semester (in a good
scenario). The Data Refreshment phase needs to be executed quickly and in the
background so as to allow users to continue using the EIS without any downtime.

For the Dimension tables, it was decided to use the MyISAM database engine,
since up to the 5.1 release offers a fair relation between data consistency and
throughput, although consistency could require some configuration on the
underlying OS. The kind of data that is being housed is not extremely critical
that needs to be kept history logs or support for transactions so MyISAM
satisfies this module's objectives.

The Fact tables use the MERGE\_MyISAM database engine, allowing the System's
Data Refreshment phase to partition the data by refreshment period (Year or
Semester) and keep it in separate tables. The main MERGE table houses the union
of all the partition tables and their indices thus achieving better performance
in some scenarios\cite{dw:mysql:boss}, for example, querying data from a single
semester, which will be a common task in the EIS. Table partitioning also allows
the databases physical files to be moved to separate disks, allowing better
performance by, for example, having the first semester table file in a
\emph{SSD} disk with a \emph{btrfs} file system partition.

The tables’ indices can also be preloaded to cache on demand, after the
Refreshment process is done, to avoid loading indices on database query
time\cite{dw:mysql:boss}.

\section{Model Management}

\subsection{OLAP}

Online Analytical Processing is a set of techniques applied to the Data
Warehouse to summarize and visualize business information in a multidimensional
perspective across multiple dimensions. Since the Data Warehouse is designed to
use a multidimensional data model, information is organized in Data Cubes and
thus enabling the EIS users to gain more insight into the data by performing
these operations:

\begin{itemize}
  \item Facts aggregation
  \item Slice and Dice across multiple Dimensions
  \item Drill-Down and Roll-Up the data from one hierarchy to another
\end{itemize}

Here are some examples of OLAP in action in an Accounting
context\cite{site:lbi}:

\begin{itemize}
  \item Comparison of sales (fact) of a product (dimension) over years
  (dimension) in the same region (dimension).
  \item How may members (fact) have opened a savings account (dimension), in USA
  branch (dimension), over a period (dimension)?
  \item How many mortgage loans (fact) have been approved in fixed mortgage
  (dimension) or Adjustable Rate Mortgage (dimension) in New York City
  (dimension), over a period (dimension)?
  \item What is the total sales value (fact) of a particular product (dimension)
  in a particular grocery store (dimension), over a period (dimension)?
  \item What is the amount spent (fact) for a particular product promotion
  (dimension) in a particular branch (dimension) or in a particular city
  (dimension), over a period (dimension)?
\end{itemize}

\subsection{Data Mining}

Data mining tools are especially appropriate for large and complex sets of data.
Through statistical or modeling techniques, data mining tools make it possible
to discover hidden trends or rules that are implicit in a large database.

Data mining tools can be applied to data from data warehouses or relational
databases. Data discovered by these tools must be validated and verified to
become operational data that can be used in the decision process.


\section{Data Visualization}

\subsection{The EIS Web Interface}

As previously mentioned, the user interface should be flexible and intuitive
enough for users without previous skills to select the indicators they want and
operate filters by several dimensions and metrics - this operation can be called
a "scenario". These "scenarios" can be saved and edited for later used by the
user.

\subsubsection{Features}

Each scenario configuration can include the following features:

\begin{itemize}
  \item Select OLAP cubes - the Objectives - for performance monitoring
  \item Generate pivot tables
  \begin{itemize}
    \item Selecting Dimensions for columns and rows
    \item Selecting measures for columns and rows
    \item “Drill-Down” data by displaying aggregated information for a member of
    a Dimension
	\item “Roll-Up” data by displaying aggregated information from ‘All’ members
	in a Dimension
	\item Swap Columns / Rows content
	\item Filter Dimensions by Value Type
	\item Highlight Dimensions with a different color, depending on Column / Row
	location
	\item Scenario Management
	\begin{itemize}
	  \item Create a new scenario
	  \item Save a scenario
	  \item Load a previous scenario
	 \end{itemize}
	 \item Export results
	 \begin{itemize}
	   \item Spreadsheet
	   \item Text file
	   \item Image files
	  \end{itemize}
  \end{itemize}
\end{itemize}

The after-development version of the interface can be seen in Figure
\ref{img:eis_ui}.

\fig{img/eis_ui.png}{The EIS interface after development}{img:eis_ui}{0.7}

\subsubsection{Pivot Tables}

PivotTables are interactive tables that can summarize large amounts of data,
using a structure and method of calculation that can be specified by the user.
It is a data analysis tool that summarizes a series of records in a concise
tabular format. They transform a raw data base, which has no reading, into a
table with multiple dynamic analysis data\cite{blog:proiete}.

They are quite common to find in spreadsheet applications beacause they are a
powerfull tool of analysis. In the next section we will focus on the User
Experience on creating and manipulating Pivot Tables.


\subsection{EIS Interface Usage}

%TODO

\section{Package Deployment}\label{sec:deployment}

\fig{img/eis_deploy.png}{Data Flow between Erasmusline, the slave and master
STATS daemon}{img:deployment}{0.7}

\subsection{UNIX Daemons}

Unix Daemons are processes that run the background, performing all sorts of
tasks and intercommunicating with other applications if needed.

The Daemons developed in PHP for the P8-STATS package generally do the following
tasks:

\begin{itemize}
  \item Periodically executes the ETL process, according to determined
  refreshment cycles.
  \item Communicate with the ErasmusLine application - for example saving ODS
  information
  \item Save user’s EIS scenarios
  \item Load EIS scenarios and perform OLAP operations on the Data Warehouse
  \item Cache EIS query results, product of OLAP operations
  \item Request data from other daemons - for example retrieve Efficacy data from
the central Data Mart
\end{itemize}

Every ErasmusLine installation has a Slave Daemon to perform these tasks. A
Master Daemon is deployed with the central Data Mart - that could be on the
Slave Daemon’s machine - and can be reached by configuring a simple settings
file in JSON format and setting the Master’s IP address. All daemons support
BSD socket and TCP-IP socket communication, supported by the PHP socket API.
BSD sockets allow less overhead in local inter-process communications
since it doesn't rely on the TCP-IP layers stack. Remote communications rely
only on TCP-IP sockets.

\subsection{Server Communication}

%TODO

\subsection{ETL / Data Refreshment Communication}
Each server has a scheduling module that triggers server tasks to be executed
from time to time or at a specific date.

At the end of each semester the Slave Servers trigger the data refreshment task
which in turn runs in the background and gathers new data from the ODS tables to
be integrated into the Data Warehouse and it's corresponding Data Marts.

The Master Server however needs to communicate with every running Slave Server
to check for new aggregated data to be integrated to it's Data Mart. The Slave
Servers periodically check for the Master Server's location and gives it their
location. The Master Server in turn saves their locations in the database. When
the time comes to refresh the Master Server's Data Mart, it calls every saved
Slave Server and asks them for the aggregated data with extra information
relevant to other Key Performance Indicators. After saving that data in the
local ODS tables, the Master Server triggers it's own data refreshment task and
integrates the new data into the Data Warehouse.


\section{Development Process}

The P8-STATS team began by analyzing the Objectives and Key Performance
Indicators based on the business rules imposed by the Erasmusline application
workflow and the working version of the application's database. This implied
studying the basic functionality of a Data Warehouse and it's implementation in
the MySQL RDBMS.

The team began to prototype a database schema in a star fashion – as proposed in
the analyzed Data Warehousing guidelines – and make a few test queries on high
volumes of fact records with dummy data. Analyzing the limitations of MySQL and
it's table engines, BTREE type indices was the only type available when using
MyISAM table engine.

Another specification assumed, was that the user will normally consult sub-sets
of data, usually by academic date. The advantages of implementing the fact
tables using the Merge Table Engine - available in MySQL – were analyzed, and
concluded that partitioning the data by year or semester would provide better
performance not only in queries – the SQL optimizer would only fetch a certain
partition, given that the user will usually analyze recent data - but also in
the ETL and Data refreshment process, allowing to avoid locking and rebuilding a
fact table's indices while aggregating new data. Also in the consulted
literature, implementing index cache size alterations and creating a separate
index cache pool for the Dimension tables would provide additional performance.
By loading the indices beforehand to hot cache these would stay in resident in
memory to provide a path to the corresponding Merge Table partitions. 

Given that the Efficacy data is to accessed by students in every institution and
assuming that the data itself is an aggregation of all institution's data, the
team decided to house Efficacy data in a central server, and leave the
Efficiency data local. That way each institution accesses their own Efficiency
data. 

To manage these tables and conduct the activities of OLAP processing, ETL and
Data Refreshment and managing a central and local Data Warehouse, it was decided
that a background process would be conveniently needed. Unix Daemons were
developed, being a common implementation in Open Source Operating Systems based
on Unix, like Linux and FreeBSD. Theses Daemons act as Servers and gateways to
access the Data Warehouse. The implementation consists of an OLAP module, an ETL
module, a JSON- RPC communication module, a caching module, a performance
profiler with logging and a Scheduling module, to trigger events such as Data
Refreshment.

While analyzing the EIS, there was the desire to create an interface that would
implement client-side processing to reduce load on the servers and only rely on
the Daemons for result delivery. Client-side processing was achieved using the
jQuery Javascript library by implementing client-side templating, via the jQuery
Tmpl plug-in, allowing us to generate HTML content from the Browser.
Communication would be made by JSON-RPC strings and background HTTP requests
like Ajax, with the specified Scenario management and query processing being
made by the local Daemon. Communication with the central Daemon and it's
database would be relayed by the local Daemon, directly delivering the result of
the central database. 

Being concerned with the accessibility guidelines, validation tools would be
used to enforce such guidelines. For example, the use of AJAX techniques are
accepted, but the user must be notified at all times of the shift of attention
in the interface.
