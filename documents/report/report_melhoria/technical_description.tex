\chapter{Technical Description} \label{ch:technical}

The P8-STATS package, the integral part of the statistical core developed by the
Portuguese team consists in developing an Executive Infortmation System,
integrated in the ErasmusLine ecosystem.

Executive Information Systems (EIS) are Information Systems that access large
amounts of business related information from various sources - internal and
external to the organization - and provides decision support tools to the
organization's senior executives.

These tools allow the executive managers to highlight patterns in the
business process by analyzing, comparing and determining trends in the provided
mediums - spreadsheets, graphic charts, pivot tables, reports, etc.

As an example of EIS use, consider the following scenario:

\begin{itemize}
\item[] Erasmus executive staff and coordinators can do a periodical check on
student enrollment information, i.e. what countries do they come from and what courses
are their main choices. Using a web browser, executive memebers can access that
information in a personalized fashion by presenting subsets of the information -
views. These views can be "drilled-down" to minute levels of information or
"rolled-up" to display a broader view.
\end{itemize}

Since executive users are not necessarily data analysts, the EIS user interface
must be as simple as possible, but without sacrificing presentation dynamics.

With these criteria in mind, the following key requirements for an EIS where
identified\cite{eis:psu}:

\begin{itemize}
  \item Cross Platform
  \item Ease of Use
  \item Limited Training
  \item Quick Response
  \item Process Large Volumes of Data
  \item Deployment Through the Web
  \item Easy Graphical Presentation Options
  \item Ability to Access Subsets of Data (Drill Down)
\end{itemize}

\section{Architecture}
An EIS can be divided into three main levels of architecture\cite{eis:dlbi}, that
covers the technology used to extract, analyze and visualize information from other data
sources with a friendly and flexible user interface. The levels discussed in the
next sections are the following:

\begin{description}
  \item[Data Management] Data gathering and transformation, it's lifecycle and
  target storage.
  \item[Model Management] How the data is transformed and retrieved for
  analytical purposes.
  \item[Data Visualization] Which visual tools are used to represent the
  underlying system architecture.
\end{description}


\section{Data Management}

The Data Management Level deals with the extraction, analysis and processing of
the internal and external data sources and passes that data through an ETL
process - Extract, Transform and Load - that organizes and aggregates the data
into the Data Warehouse. The use of ETL and Data Warehouses can be more
efficient to the EIS due to it's data treatment and Warehouse database model
schemata.

In this project's context, the sole Data Source for the Data Management level is
the main Erasmusline database that provides information for 
\begin{itemize}
	\item The Students
	\item The Higher Education Institutions
	\item The lectured Courses
	\item Student Forms
	\item Exams
\end{itemize}

It is this data that will pass through the ETL stage.

\subsection{ETL process}

The ETL process parses the data and performs these possible
operations\cite{wiki:etl}:
\begin{itemize}
  \item Translation into database compatible rows / columns
  \item Translate code values
  \item Remove / Add / Translate code values
  \item Change data encoding
  \item Unify string value variations - i.e. 'Mr','Mister'; '1','One'; etc.
  \item Sorting values
  \item Data validation
  \item Summarize data
  \item Join data from various sources
  \item Generating surrogate keys
\end{itemize}

Also, ETL processes can make use of temporary database tables for intermediate
processing, called Operational Data Stores (ODS). These tables hold historical
data that can me mined for statistical information to be included in the Data
Warehouse.

\fig{img/etl_workflow.png}{ETL Process Workflow}{img:etl_workflow}{0.8}

For the P8-STATS package, the ETL process - Figure \ref{img:etl_workflow} - will
be executed either by an event trigger - saving data in the ODS’s when a certain
workflow phase is reached - and periodically - part of the data refreshment
process to aggregate new information to the Data Warehouse.

\subsubsection{Data Refreshment}

This phase of the ETL process is comprised of design choices, like data
structures, techniques to update and optimize the flow of information from the
data sources to the Data Warehouse and update cycle.

Based on the statistical overview document of the Erasmus Program of 2010 – in
the Annexes – it was verified that with the large volume of students mobility,
having a completely centralized database wouldn't be viable in terms of database
synchronization and efficiency - 198523 students got mobilized by 2747
Institutions in the academic year 2008/2009.

The process begins with the loading of information from the Operational Data
Stores.

\paragraph{Loading}

The information sent to the ETL component will be fetched from the application
data sources while other subsets of information are already stored in the ODS’s
or Metadata tables - these hold information previously gathered by the business
requisites - which is going to be processed by the ETL component and integrated
into the Data Warehouse.

To aggregate and integrate data to be viewed in the EIS, an update cycle for the
Data Warehouse must be determined. The information models that need to be
gathered over time are noted in the following table:  

\tab{img/refresh_cycle.png}{EIS update cycle for the Data Refreshment
phase}{tab:refresh_cycle}{0.7}

After the Process Efficiency data is integrated into the Institution’s local
Data Mart - a local database, part of the Data Warehouse - the Efficacy process
gathers the data from the various Institution’s Data Marts and additional data
to aggregate and integrate new data into the central Data Mart.

More on this discussion in the Deployment section, page
\pageref{sec:deployment} of this report.

\paragraph{Aggregation}

The aggregation process of Student Application data fetches the information from
the ODS database - Figure \ref{img:ods_tables} - and integrates those
values in the Data Warehouse database.

Some technical aspects have to be considered in the loading and aggregation of
data sources:

\begin{itemize}
  \item Keep an historical record of information already loaded, so as not to
  fetch the same data twice. This is a achieved with a caching mechanism, to
  prevent repetitive DB queries for example.
  \item The Data Warehouse Database Schema must be optimized for query
  performance, that includes:
  \begin{itemize}
    \item Which indices are needed
    \item The Index type vs the Data type
    \item Which Database/Table engine
   \end{itemize}
\end{itemize}

The technical considerations and decision are explained in detail on page
\pageref{ssec:mysql_merge} - \nameref{ssec:mysql_merge}.

\fig{img/ods_tables.png}{ODS and Metadata DB Schema}{img:ods_tables}{0.7}

\newpage
\subsection{Data Warehouse}

A Data Warehouse holds the processed information into a database designed
properly for the retrieval of large amounts of data. The database structure is
composed of fact tables and dimension tables, connected by their keys in a
\textbf{Star} or \textbf{Snowflake} schema\cite{dwtk}.

A Fact table holds facts - an aggregation of contextualized fields and numerical
values for measurement\cite{dwtk}.

A Dimension table holds the various combinations of a given dimension, often
identified by a part of the composite key in the fact table and serves as a
means to restrict and summarize the information contained in the fact table
("roll-up" and "drill-down" operations)\cite{dwtk}.

Figure \ref{img:dw_schema} shows the final Data Warehouse database schema used
in the ErasmusLine P8-STATS package.

This schema follows the Star schema concept, not as normalized as the Snowflake
schema but with better performance\cite{dwtk} - Snowflake schemas are in third
normal form - since it doesn't need the overhead of looking up additinal tables
in the OLAP module.

The following analysis breakdown in the next section describes which data can be
mined given the project Specifications and the ErasmusLine business information,
and what Key Performance Indicators can be obtained, measured and stored in the
Data Warehouse.

\newpage
\fig{img/dw_schema.png}{Data Warehouse Database Schema}{img:dw_schema}{0.7}
\newpage

\subsubsection{Data Warehouse Statistical Specifications}

Given the initial instructions provided by the MUTW 2011 Student's Project
Specification document, the main analytical objectives for the P8-STATS package
are the following:

\begin{itemize}
  \item Efficiency
  \item Efficacy
\end{itemize}

Given the state and final content of the main \emph{ErasmusLine} database, the
following Key Performance Indicators can be established:

\paragraph{Efficiency}
\begin{itemize}
  \item Student participation
  \item Response time between Process phases
  \item Lodging availability
\end{itemize}

\paragraph{Efficacy}
\begin{itemize}
  \item Student Applications
  \item Student credits
  \item Student grades
\end{itemize}

Taking these KPI's into account, Table \ref{tab:measures} details the
Measures that could be determined.

\tab{img/tab_measures.png}{Defined Measures for the Data
Warehouse}{tab:measures}{0.6}

A cross analysis between Measures and KPI's to determine which measures would
be used more than once, shown in Table \ref{tab:kpi_measures}.

\tab{img/tab_kpi_measures.png}{Cross referencing KPI's with
Measures}{tab:kpi_measures}{0.7}

In Table \ref{tab:dimensions} are the Dimensions gathered from the
specifications document that cross reference with the final ErasmusLine
application database.

\tab{img/tab_dimensions.png}{The Dimension Classification, their levels of
detail and examples}{tab:dimensions}{0.7}

\newpage
To match which Dimension would be present on each KPI, Table \ref{tab:kpi_dim}
shows their relations.

\tab{img/tab_kpi_dim.png}{Cross reference between KPI's and
Dimensions}{tab:kpi_dim}{0.7}

With these tables into account it was possible to determine the Data Warehouse
Database schema illustrated in Figure \ref{img:dw_schema}.

Additional analysis was made to make the database schema scalable and perform
well under high server loads. The following section explains the choices of
the underlying database architecture given the system's constraints and
possibilities.

\subsubsection{MySQL and Merge Tables}\label{ssec:mysql_merge}


